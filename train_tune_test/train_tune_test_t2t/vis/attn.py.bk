import json
import os
import sys
import yaml
from seq2seq.data import vocab
import IPython.display as display

import numpy as np

vis_html = """
  <span style="user-select:none">
    Layer: <select id="layer"></select>
    Attention: <select id="att_type">
      <option value="inp1d_inp1d">Input1d - input1d</option>
      <option value="inp_inp">Input - Input</option>
      <option value="inp_out">Input - Output</option>
      <option value="out_out">Output - Output</option>
    </select>
  </span>
  <div id='vis'></div>
"""

# include the js file for plotting visualization
__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))
vis_js = open(os.path.join(__location__, 'attn.js')).read()

def _show_attention(att_json):
  display.display(display.HTML(vis_html))
  display.display(display.Javascript('window.attention = %s' % att_json))
  display.display(display.Javascript(vis_js))

def show(inp_text, out_text, enc_atts, dec_atts, encdec_atts, model_dir):
    attention = _get_attention(
        inp_text, 
        out_text, 
        enc_atts,
        dec_atts,
        encdec_atts,
        model_dir)
    att_json = json.dumps(attention)
    _show_attention(att_json)

#def show(inp_text, out_text, enc_att_1d, enc_atts, dec_atts, encdec_atts, model_dir):
#    attention = _get_attention(
#        inp_text, 
#        out_text, 
#        enc_att_1d, 
#        enc_atts, 
#        dec_atts, 
#        encdec_atts,
#        model_dir)
#    att_json = json.dumps(attention)
#    _show_attention(att_json)

def _get_attention(inp_text, out_text, enc_att_1d, enc_atts, dec_atts, encdec_atts, model_dir):
    '''
    [num_layers, batch_size, num_heads, enc/dec_length, enc/dec_length]
    '''
    def get_attentions(get_attention_fn):
        num_layers = len(enc_atts)
        attentions = []
        for i in range(num_layers):
            attentions.append(get_attention_fn(i))
        return attentions

    #def get_full_attention(layer):
    #    """Get the full input+output - input+output attentions."""
    #    enc_att_1d = enc_att_1d[0]
    #    enc_att = enc_atts[layer][0]
    #    encdec_att = encdec_atts[layer][0]
    #    dec_att = dec_atts[layer][0]

    #    enc_att_1d = np.transpose(enc_att_1d, [0, 2, 1])
    #    enc_att = np.transpose(enc_att, [0, 2, 1])
    #    encdec_att = np.transpose(encdec_att, [0, 2, 1])
    #    dec_att = np.transpose(dec_att, [0, 2, 1])

    #    # [heads, query_length, memory_length]
    #    enc_1d_length = enc_att_1d.shape[1]
    #    enc_length = enc_att.shape[1]
    #    dec_length = dec_att.shape[1]
    #    num_heads = enc_att.shape[0]

    #    first = np.concatenate([enc_att, encdec_att], axis=2)
    #    second = np.concatenate(
    #        [np.zeros((num_heads, dec_length, enc_length)), dec_att], axis=2)
    #    full_att = np.concatenate([first, second], axis=1)
    #    return [attn_mat.T.tolist() for attn_mat in full_att]

    def get_1d_attention(layer):
        att = np.transpose(enc_att_1d[0], (0, 2, 1))
        return [ha.T.tolist() for ha in att]

    def get_inp_inp_attention(layer):
        att = np.transpose(enc_atts[layer][0], (0, 2, 1))
        return [ha.T.tolist() for ha in att]

    def get_out_inp_attention(layer):
        att = np.transpose(encdec_atts[layer][0], (0, 2, 1))
        return [ha.T.tolist() for ha in att]

    def get_out_out_attention(layer):
        att = np.transpose(dec_atts[layer][0], (0, 2, 1))
        return [ha.T.tolist() for ha in att]

    #inp_text_1d = list(np.reshape(np.stack(np.tile(inp_text, (4,1)), 1), -1))
    config_file = os.path.join(model_dir, "reg_config", "config.yml")
    config = yaml.load(open(config_file))
    lexicon_dict_file = config["bilingual_lexicon"]
    lexicon_dict = vocab.get_lexicon_dict(lexicon_dict_file)
    inp_text_1d = map_to_lex_cap(inp_text, lexicon_dict)

    attentions = {
        'inp1d_inp1d': {
            'att': get_attentions(get_1d_attention),
            'top_text': inp_text_1d,
            'bot_text': inp_text_1d,
        },
        'inp_inp': {
            'att': get_attentions(get_inp_inp_attention),
            'top_text': inp_text,
            'bot_text': inp_text,
        },
        'inp_out': {
            'att': get_attentions(get_out_inp_attention),
            'top_text': inp_text,
            'bot_text': out_text,
        },
        'out_out': {
            'att': get_attentions(get_out_out_attention),
            'top_text': out_text,
            'bot_text': out_text,
        },
    }

    return attentions
    
def map_to_lex_cap(token_list, lexicon_dict):
    ret = []
    for token in token_list:

        ret.extend(pad_to_lex_cap(lexicon_dict[token], 4) if token in lexicon_dict else ["UNK"]*4)
    return ret

def pad_to_lex_cap(lex_list, lex_cap):
    return np.tile(lex_list, int(lex_cap/len(lex_list))+1)[:lex_cap]



        

