{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import theanolm\n",
    "import os\n",
    "import os.path\n",
    "import heapq\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "sys.path.insert(0, '/home/ec2-user/kklab/Projects/lrlp/scripts/oov_translate')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### language model directory\n",
    "tmp_dir = exp_dir+\"oov_trans_neural/\"\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/; /home/ec2-user/kklab/src/brown-cluster/wcluster --text /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/elisa.il3-eng.train.y1r1.v2.true.eng.txt --c 50\n",
      "theanolm train /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/lm_50_lstm1500_1.0_nesterov_in_domain --training-set /home/ec2-user/kklab/data/ELISA/evals/y1/JMIST/elisa.il3.package.y1r1.v2/train/elisa.il3-eng.train.y1r1.v2.true.eng --validation-file /home/ec2-user/kklab/data/ELISA/evals/y1/JMIST/elisa.il3.package.y1r1.v2/dev/elisa.il3-eng.dev.y1r1.v2.true.eng --vocabulary /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/elisa.il3-eng.train.y1r1.v2.true.eng-c50-p1.out/paths3 --vocabulary-format classes  --architecture lstm1500 --learning-rate 1.0 --optimization-method nesterov --stopping-criterion no-improvement --validation-frequency 8 --patience 4\n"
     ]
    }
   ],
   "source": [
    "### hyper parameters to this method, to be used as parameters of this functions \n",
    "cluster_size = 50\n",
    "net_arch = \"lstm1500\"\n",
    "learning_rate = 1.0\n",
    "optimization_method = \"nesterov\"\n",
    "\n",
    "### language_model\n",
    "language_model = \"{}_{}_{}_{}\".format(cluster_size, \\\n",
    "                                      net_arch.split(\"/\")[-1], \\\n",
    "                                      learning_rate, \\\n",
    "                                      optimization_method)\n",
    "\n",
    "lm_in_domain_path = tmp_dir+\"lm_\"+language_model+\"_in_domain\"\n",
    "train_in_domain_tmp = tmp_dir+\"train_in_domain.txt\"\n",
    "\n",
    "if not os.path.exists(train_in_domain_tmp):\n",
    "    shutil.copyfile(train_in_domain, train_in_domain_tmp)\n",
    "\n",
    "### word clustering\n",
    "print(\"cd \"+tmp_dir+\"; \"+\\\n",
    "      wcluster+\\\n",
    "      \" --text \"+train_in_domain_tmp+\\\n",
    "      \" --c \"+str(cluster_size))\n",
    "\n",
    "### adapt the output of wcluster to something that theanoLM recognizes as vocab\n",
    "vocab_path = tmp_dir+\"train_in_domain-c\"+str(cluster_size)+\"-p1.out/paths\"\n",
    "if not os.path.exists(vocab_path+\"3\"):\n",
    "    sh(\"cut -f1,2 \"+vocab_path+\" > \"+vocab_path+\"2\")\n",
    "    sh(\"awk -F $'\\t' ' { t = $1; $1 = $2; $2 = t; print; } ' OFS=$'\\t' \"+\\\n",
    "       vocab_path+\"2\"+\\\n",
    "       \" > \"+vocab_path+\"3\")\n",
    "\n",
    "print(\"theanolm train \"+lm_in_domain_path+\\\n",
    "      \" --training-set \"+train_in_domain+\\\n",
    "      \" --validation-file \"+dev_in_domain+\\\n",
    "      \" --vocabulary \"+vocab_path+\"3\"+\\\n",
    "      \" --vocabulary-format classes \"+\\\n",
    "      \" --architecture \"+net_arch+\\\n",
    "      \" --learning-rate \"+str(learning_rate)+\\\n",
    "      \" --optimization-method \"+optimization_method+\\\n",
    "      \" --stopping-criterion no-improvement\"+\\\n",
    "      \" --validation-frequency 8\"+\\\n",
    "      \" --patience 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file exists at: /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain\n",
      "total_num_in_domain: 99005\n",
      "total_num_non_domain: 99005\n",
      "cd /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/; /home/ec2-user/kklab/src/brown-cluster/wcluster --text /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain.txt --c 50\n",
      "theanolm train /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/lm_50_lstm1500_1.0_nesterov_non_domain --training-set /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain --validation-file /home/ec2-user/kklab/data/ELISA/evals/y1/JMIST/elisa.il3.package.y1r1.v2/dev/elisa.il3-eng.dev.y1r1.v2.true.eng --vocabulary /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain-c50-p1.out/paths3 --vocabulary-format classes  --architecture lstm1500 --learning-rate 1.0 --optimization-method nesterov --stopping-criterion no-improvement --validation-frequency 8 --patience 4\n"
     ]
    }
   ],
   "source": [
    "train_non_domain = tmp_dir+\"train_non_domain\"\n",
    "total_num_in_domain = get_file_length(train_in_domain)\n",
    "if not os.path.exists(train_non_domain):\n",
    "    total_num_non_domain = random_sample(train_non_domain_all, \\\n",
    "                                         total_num_in_domain, \\\n",
    "                                         train_non_domain)\n",
    "else:\n",
    "    print(\"Sample file exists at: \"+train_non_domain)\n",
    "    total_num_non_domain = get_file_length(train_non_domain)\n",
    "print(\"total_num_in_domain: \"+str(total_num_in_domain))\n",
    "print(\"total_num_non_domain: \"+str(total_num_non_domain))\n",
    "\n",
    "lm_non_domain_path = tmp_dir+\"lm_\"+language_model+\"_non_domain\"\n",
    "train_non_domain_tmp = tmp_dir+\"train_non_domain.txt\"\n",
    "\n",
    "if not os.path.exists(train_non_domain_tmp):\n",
    "    shutil.copyfile(train_non_domain, train_non_domain_tmp)\n",
    "\n",
    "### word clustering\n",
    "print(\"cd \"+tmp_dir+\"; \"+\\\n",
    "      wcluster+\\\n",
    "      \" --text \"+train_non_domain_tmp+\\\n",
    "      \" --c \"+str(cluster_size))\n",
    "\n",
    "### adapt the output of wcluster to something that theanoLM recognizes as vocab\n",
    "vocab_path = tmp_dir+\"train_non_domain-c\"+str(cluster_size)+\"-p1.out/paths\"\n",
    "if not os.path.exists(vocab_path+\"3\"):\n",
    "    sh(\"cut -f1,2 \"+vocab_path+\" > \"+vocab_path+\"2\")\n",
    "    sh(\"awk -F $'\\t' ' { t = $1; $1 = $2; $2 = t; print; } ' OFS=$'\\t' \"+\\\n",
    "       vocab_path+\"2\"+\\\n",
    "       \" > \"+vocab_path+\"3\")\n",
    "\n",
    "print(\"theanolm train \"+lm_non_domain_path+\\\n",
    "      \" --training-set \"+train_non_domain+\\\n",
    "      \" --validation-file \"+dev_in_domain+\\\n",
    "      \" --vocabulary \"+vocab_path+\"3\"+\\\n",
    "      \" --vocabulary-format classes \"+\\\n",
    "      \" --architecture \"+net_arch+\\\n",
    "      \" --learning-rate \"+str(learning_rate)+\\\n",
    "      \" --optimization-method \"+optimization_method+\\\n",
    "      \" --stopping-criterion no-improvement\"+\\\n",
    "      \" --validation-frequency 8\"+\\\n",
    "      \" --patience 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'list' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b0f9ce609045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mall_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mcross_entropy_in_domain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_in_domain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scored every sentence in non-domain training using in-domain lm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mcross_entropy_non_domain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_non_domain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b0f9ce609045>\u001b[0m in \u001b[0;36mscore_sent\u001b[0;34m(lm_model, all_sent, prob_or_entropy)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     '''\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"theanolm score \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlm_model\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mall_sent\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" --output word-scores --log-base 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'list' object to str implicitly"
     ]
    }
   ],
   "source": [
    "def find_prob(res_list):\n",
    "    ptr = -2\n",
    "    while 'log(p(</s> |' not in res_list[ptr]:\n",
    "        ptr -= 1\n",
    "    res = res_list[ptr]\n",
    "    return float(res.split(' = ')[-1])\n",
    "\n",
    "def score_sent(lm_model, all_sent, prob_or_entropy):\n",
    "    '''\n",
    "    param:\n",
    "        lm_model: path to lm file\n",
    "        all_sent: path to file of all sentences to be scored\n",
    "    return:\n",
    "        scores: list of scores for sentences\n",
    "    '''\n",
    "    stdout, stderr = sh(\"theanolm score \"+lm_model+\" \"+all_sent+\" --output word-scores --log-base 10\")\n",
    "    stdout = stdout.split(\"\\n\\n\")\n",
    "    stdout = stdout[0:-1]\n",
    "    probs = [find_prob(item.split('\\n')) for item in stdout]\n",
    "    if prob_or_entropy == \"prob\":\n",
    "        return probs\n",
    "    elif prob_or_entropy == \"entropy\":\n",
    "        return [-probs[i]*1.0/len(all_sent[i].split(' ')) for i in range(len(probs))]\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "### heap\n",
    "denominator = 2.0\n",
    "total_num_non_domain_all = get_file_length(train_non_domain_all)\n",
    "cutoff_num_non_domain = total_num_non_domain_all/denominator\n",
    "\n",
    "cross_entropy_in_domain = score_sent(lm_in_domain_path, \\\n",
    "                                     train_non_domain_all, \\\n",
    "                                     \"entropy\")\n",
    "print(\"scored every sentence in non-domain training using in-domain lm\")\n",
    "cross_entropy_non_domain = score_sent(lm_non_domain_path, \\\n",
    "                                      train_non_domain_all, \\\n",
    "                                      \"entropy\")\n",
    "print(\"scored every sentence in non-domain training using non-domain lm\")\n",
    "cross_entropy_diff = [-(cross_entropy_in_domain[i]-cross_entropy_non_domain[i]) for i in range(len(cross_entropy_in_domain))]\n",
    "\n",
    "score_sent = []\n",
    "with open(train_non_domain_all) as f:\n",
    "for i in range(len(all_sent)):\n",
    "    if len(score_sent) < cutoff_num_non_domain:\n",
    "        heapq.heappush(score_sent, (cross_entropy_diff[i], all_sent[i]))\n",
    "    else:\n",
    "        spilled = heapq.heappushpop(score_sent, (cross_entropy_diff[i], all_sent[i]))\n",
    "print(\"1/\"+str(denominator)+\" of the of non-domain training data has been loaded to heap.\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------- hyperparameters specific to this method --------\n",
    "### ug_dict or eng_vocab\n",
    "candidate_source = \"ug_dict\"\n",
    "### True or False, only meaningful when candidate_source == ug_dict\n",
    "add_aligned_oov = True\n",
    "# ### neural\n",
    "# language_model = \"neural\"\n",
    "\n",
    "# # -------- write --------\n",
    "# res_file = \".\".join([tra_file,\\\n",
    "#                      \"oovtranslated\",\\\n",
    "#                      candidate_source,\\\n",
    "#                      \"lm_\"+language_model])\n",
    "# if candidate_source is \"ug_dict\":\n",
    "#     if add_aligned_oov:\n",
    "#         res_file = \".\".join([tra_file,\\\n",
    "#                              \"oovtranslated\",\\\n",
    "#                              candidate_source+\"_withAlignedOov\",\\\n",
    "#                              \"lm_\"+language_model])\n",
    "#     else:\n",
    "#         res_file = \".\".join([tra_file,\\\n",
    "#                              \"oovtranslated\",\\\n",
    "#                              candidate_source+\"_withoutAlignedOov\",\\\n",
    "#                              \"lm_\"+language_model])\n",
    "\n",
    "# -------- translate --------\n",
    "#oov_trans_neural(candidate_source, add_aligned_oov, language_model, res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
