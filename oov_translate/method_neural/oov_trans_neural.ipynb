{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import theanolm\n",
    "import os\n",
    "import os.path\n",
    "import heapq\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "sys.path.insert(0, '/home/ec2-user/kklab/Projects/lrlp/scripts/oov_translate')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### language model directory\n",
    "tmp_dir = exp_dir+\"oov_trans_neural/\"\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/; /home/ec2-user/kklab/src/brown-cluster/wcluster --text /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/elisa.il3-eng.train.y1r1.v2.true.eng.txt --c 50\n",
      "theanolm train /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/lm_50_lstm1500_1.0_nesterov_in_domain --training-set /home/ec2-user/kklab/data/ELISA/evals/y1/JMIST/elisa.il3.package.y1r1.v2/train/elisa.il3-eng.train.y1r1.v2.true.eng --validation-file /home/ec2-user/kklab/data/ELISA/evals/y1/JMIST/elisa.il3.package.y1r1.v2/dev/elisa.il3-eng.dev.y1r1.v2.true.eng --vocabulary /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/elisa.il3-eng.train.y1r1.v2.true.eng-c50-p1.out/paths3 --vocabulary-format classes  --architecture lstm1500 --learning-rate 1.0 --optimization-method nesterov --stopping-criterion no-improvement --validation-frequency 8 --patience 4\n"
     ]
    }
   ],
   "source": [
    "### hyper parameters to this method, to be used as parameters of this functions \n",
    "cluster_size = 50\n",
    "net_arch = \"lstm1500\"\n",
    "learning_rate = 1.0\n",
    "optimization_method = \"nesterov\"\n",
    "\n",
    "### language_model\n",
    "language_model = \"{}_{}_{}_{}\".format(cluster_size, \\\n",
    "                                      net_arch.split(\"/\")[-1], \\\n",
    "                                      learning_rate, \\\n",
    "                                      optimization_method)\n",
    "\n",
    "lm_in_domain_path = tmp_dir+\"lm_\"+language_model+\"_in_domain\"\n",
    "train_in_domain_tmp = tmp_dir+\"train_in_domain.txt\"\n",
    "\n",
    "if not os.path.exists(train_in_domain_tmp):\n",
    "    shutil.copyfile(train_in_domain, train_in_domain_tmp)\n",
    "\n",
    "### word clustering\n",
    "print(\"cd \"+tmp_dir+\"; \"+\\\n",
    "      wcluster+\\\n",
    "      \" --text \"+train_in_domain_tmp+\\\n",
    "      \" --c \"+str(cluster_size))\n",
    "\n",
    "### adapt the output of wcluster to something that theanoLM recognizes as vocab\n",
    "vocab_path = tmp_dir+\"train_in_domain-c\"+str(cluster_size)+\"-p1.out/paths\"\n",
    "if not os.path.exists(vocab_path+\"3\"):\n",
    "    sh(\"cut -f1,2 \"+vocab_path+\" > \"+vocab_path+\"2\")\n",
    "    sh(\"awk -F $'\\t' ' { t = $1; $1 = $2; $2 = t; print; } ' OFS=$'\\t' \"+\\\n",
    "       vocab_path+\"2\"+\\\n",
    "       \" > \"+vocab_path+\"3\")\n",
    "\n",
    "print(\"theanolm train \"+lm_in_domain_path+\\\n",
    "      \" --training-set \"+train_in_domain+\\\n",
    "      \" --validation-file \"+dev_in_domain+\\\n",
    "      \" --vocabulary \"+vocab_path+\"3\"+\\\n",
    "      \" --vocabulary-format classes \"+\\\n",
    "      \" --architecture \"+net_arch+\\\n",
    "      \" --learning-rate \"+str(learning_rate)+\\\n",
    "      \" --optimization-method \"+optimization_method+\\\n",
    "      \" --stopping-criterion no-improvement\"+\\\n",
    "      \" --validation-frequency 8\"+\\\n",
    "      \" --patience 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file exists at: /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain\n",
      "total_num_in_domain: 99005\n",
      "total_num_non_domain: 99005\n",
      "cd /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/; /home/ec2-user/kklab/src/brown-cluster/wcluster --text /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain.txt --c 50\n",
      "theanolm train /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/lm_50_lstm1500_1.0_nesterov_non_domain --training-set /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain --validation-file /home/ec2-user/kklab/data/ELISA/evals/y1/JMIST/elisa.il3.package.y1r1.v2/dev/elisa.il3-eng.dev.y1r1.v2.true.eng --vocabulary /home/ec2-user/kklab/Projects/lrlp/experiment_elisa.il3-eng.y1r1.v2/oov_trans_neural/train_non_domain-c50-p1.out/paths3 --vocabulary-format classes  --architecture lstm1500 --learning-rate 1.0 --optimization-method nesterov --stopping-criterion no-improvement --validation-frequency 8 --patience 4\n"
     ]
    }
   ],
   "source": [
    "train_non_domain = tmp_dir+\"train_non_domain\"\n",
    "total_num_in_domain = get_file_length(train_in_domain)\n",
    "if not os.path.exists(train_non_domain):\n",
    "    total_num_non_domain = random_sample(train_non_domain_all, \\\n",
    "                                         total_num_in_domain, \\\n",
    "                                         train_non_domain)\n",
    "else:\n",
    "    print(\"Sample file exists at: \"+train_non_domain)\n",
    "    total_num_non_domain = get_file_length(train_non_domain)\n",
    "print(\"total_num_in_domain: \"+str(total_num_in_domain))\n",
    "print(\"total_num_non_domain: \"+str(total_num_non_domain))\n",
    "\n",
    "lm_non_domain_path = tmp_dir+\"lm_\"+language_model+\"_non_domain\"\n",
    "train_non_domain_tmp = tmp_dir+\"train_non_domain.txt\"\n",
    "\n",
    "if not os.path.exists(train_non_domain_tmp):\n",
    "    shutil.copyfile(train_non_domain, train_non_domain_tmp)\n",
    "\n",
    "### word clustering\n",
    "print(\"cd \"+tmp_dir+\"; \"+\\\n",
    "      wcluster+\\\n",
    "      \" --text \"+train_non_domain_tmp+\\\n",
    "      \" --c \"+str(cluster_size))\n",
    "\n",
    "### adapt the output of wcluster to something that theanoLM recognizes as vocab\n",
    "vocab_path = tmp_dir+\"train_non_domain-c\"+str(cluster_size)+\"-p1.out/paths\"\n",
    "if not os.path.exists(vocab_path+\"3\"):\n",
    "    sh(\"cut -f1,2 \"+vocab_path+\" > \"+vocab_path+\"2\")\n",
    "    sh(\"awk -F $'\\t' ' { t = $1; $1 = $2; $2 = t; print; } ' OFS=$'\\t' \"+\\\n",
    "       vocab_path+\"2\"+\\\n",
    "       \" > \"+vocab_path+\"3\")\n",
    "\n",
    "print(\"theanolm train \"+lm_non_domain_path+\\\n",
    "      \" --training-set \"+train_non_domain+\\\n",
    "      \" --validation-file \"+dev_in_domain+\\\n",
    "      \" --vocabulary \"+vocab_path+\"3\"+\\\n",
    "      \" --vocabulary-format classes \"+\\\n",
    "      \" --architecture \"+net_arch+\\\n",
    "      \" --learning-rate \"+str(learning_rate)+\\\n",
    "      \" --optimization-method \"+optimization_method+\\\n",
    "      \" --stopping-criterion no-improvement\"+\\\n",
    "      \" --validation-frequency 8\"+\\\n",
    "      \" --patience 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring every sentence in non-domain training using in-domain lm...\n",
      "^C\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def score_sent(lm_model, all_sent, prob_or_entropy):\n",
    "    '''\n",
    "    param:\n",
    "        lm_model: path to lm file\n",
    "        all_sent: path to file of all sentences to be scored\n",
    "    return:\n",
    "        scores: list of scores for sentences\n",
    "    '''\n",
    "    cmd = \"theanolm score \"+lm_model+\" \"+all_sent+\" --output word-scores --log-base 10\"\n",
    "    stdout = !($cmd)\n",
    "    probs = []\n",
    "    for item in stdout:\n",
    "        if 'log(p(</s> |' in item:\n",
    "            probs.append(float(item.split('=')[-1]))\n",
    "    print(len(probs))\n",
    "#     if prob_or_entropy == \"prob\":\n",
    "#         return probs\n",
    "#     elif prob_or_entropy == \"entropy\":\n",
    "#         cross_ent = []\n",
    "#         ctr = 0\n",
    "#         with open(all_sent) as f:\n",
    "#             for sent in f:\n",
    "#                 cross_ent.append(-probs[ctr]*1.0/len(sent.split(' ')))\n",
    "#                 ctr += 1\n",
    "#         return cross_ent\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "### heap\n",
    "denominator = 2.0\n",
    "total_num_non_domain_all = get_file_length(train_non_domain_all)\n",
    "cutoff_num_non_domain = total_num_non_domain_all/denominator\n",
    "print(\"scoring every sentence in non-domain training using in-domain lm...\")\n",
    "#cross_entropy_in_domain = \n",
    "score_sent(lm_in_domain_path, \\\n",
    "                                     train_non_domain_all, \\\n",
    "                                     \"entropy\")\n",
    "# print(\"scoring every sentence in non-domain training using non-domain lm...\")\n",
    "# cross_entropy_non_domain = score_sent(lm_non_domain_path, \\\n",
    "#                                       train_non_domain_all, \\\n",
    "#                                       \"entropy\")\n",
    "# cross_entropy_diff = [-(cross_entropy_in_domain[i]-cross_entropy_non_domain[i]) for i in range(len(cross_entropy_in_domain))]\n",
    "# print(\"difference in cross entropy is obtained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_sent = []\n",
    "with open(train_non_domain_all) as f:\n",
    "    for i in range(len(all_sent)):\n",
    "        if len(score_sent) < cutoff_num_non_domain:\n",
    "            heapq.heappush(score_sent, (cross_entropy_diff[i], all_sent[i]))\n",
    "        else:\n",
    "            spilled = heapq.heappushpop(score_sent, (cross_entropy_diff[i], all_sent[i]))\n",
    "print(\"1/\"+str(denominator)+\" of the of non-domain training data has been loaded to heap.\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elisa.il3-eng.train.y1r1.v2.true-c50-p1.out', 'elisa.il3-eng.train.y1r1.v2.true.eng', 'oov_trans_neural.ipynb']\n"
     ]
    }
   ],
   "source": [
    "a = !\"ls\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------- hyperparameters specific to this method --------\n",
    "### ug_dict or eng_vocab\n",
    "candidate_source = \"ug_dict\"\n",
    "### True or False, only meaningful when candidate_source == ug_dict\n",
    "add_aligned_oov = True\n",
    "# ### neural\n",
    "# language_model = \"neural\"\n",
    "\n",
    "# # -------- write --------\n",
    "# res_file = \".\".join([tra_file,\\\n",
    "#                      \"oovtranslated\",\\\n",
    "#                      candidate_source,\\\n",
    "#                      \"lm_\"+language_model])\n",
    "# if candidate_source is \"ug_dict\":\n",
    "#     if add_aligned_oov:\n",
    "#         res_file = \".\".join([tra_file,\\\n",
    "#                              \"oovtranslated\",\\\n",
    "#                              candidate_source+\"_withAlignedOov\",\\\n",
    "#                              \"lm_\"+language_model])\n",
    "#     else:\n",
    "#         res_file = \".\".join([tra_file,\\\n",
    "#                              \"oovtranslated\",\\\n",
    "#                              candidate_source+\"_withoutAlignedOov\",\\\n",
    "#                              \"lm_\"+language_model])\n",
    "\n",
    "# -------- translate --------\n",
    "#oov_trans_neural(candidate_source, add_aligned_oov, language_model, res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
