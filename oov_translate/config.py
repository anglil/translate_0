import os


### dclm, ngram, pagerank, pmi, bound
oov_trans_method = "dclm"
oov_trans_method = os.environ["oov_trans"]

s = os.environ['s']
t = os.environ['t']
st = os.environ['st']
yrv = os.environ['yrv']

home_dir = os.environ['home_dir']+"/"
data_dir = home_dir+"data/"
src_dir = home_dir+"src/"
corpus_dir = os.environ['corpus_dir']+"/"
dataset_name = os.environ['dataset_name']
exp_dir = os.environ['exp_dir']+"/"

mt = os.environ['mt']
mt = "" if mt == "_elisa" else "_"+mt
src_label = os.environ["src_label"]
ref_label = os.environ["ref_label"]
raw = "" if mt == "_elisa" else "_raw"

### oov translation model files, intermediate files generated by the chosen method
tmp_dir = exp_dir+"oov_trans_"+oov_trans_method+"/"

roman_ab = {"som", "yor", "eng", "rus", "il6", "vie", "hau"}
lang_code = {"amh":"am","som":"so","yor":"yo","il3":"ug","hau":"ha","eng":"en","ben":"bn"}

# -------- output oov translation result --------
res_dir = exp_dir+"translation/"

### source and target (reference)
train_src_file = os.path.join(res_dir, "train", ".".join([src_label+raw,s,"train",yrv]))
train_ref_file = os.path.join(res_dir, "train", ".".join([ref_label+raw,t,"train",yrv]))
dev_src_file = os.path.join(res_dir, "dev", ".".join([src_label+raw,s,"dev",yrv]))
dev_ref_file = os.path.join(res_dir, "dev", ".".join([ref_label+raw,t,"dev",yrv]))
test_src_file = os.path.join(res_dir, "test", ".".join([src_label+raw,s,"test",yrv]))
test_ref_file = os.path.join(res_dir, "test", ".".join([ref_label+raw,t,"test",yrv]))

### hypothesis (1-best translation with oov)
dev_1best_file = os.path.join(res_dir, "dev", ".".join(["onebest"+mt,t,"dev",yrv]))
test_1best_file = os.path.join(res_dir, "test", ".".join(["onebest"+mt,t,"test",yrv]))

### hypothesis (n-best translation with oov)
dev_nbest_file = os.path.join(res_dir, "dev", "n_best", ".".join(["nbest"+mt,t,"dev",yrv]))
test_nbest_file = os.path.join(res_dir, "test", "n_best", ".".join(["nbest"+mt,t,"test",yrv]))

### oov words (format: {pos: pos:})
dev_oov_file = os.path.join(res_dir, "dev", "oov", ".".join(["oov"+mt,t,"dev",yrv]))
test_oov_file = os.path.join(res_dir, "test", "oov", ".".join(["oov"+mt,t,"test",yrv]))



# -------- external software and dataset directories --------

bleu_getter = os.environ["bleu_getter"]

### oov candidate/lexicon stuff
oov_candidates_dir = data_dir+"lorelei/for-angli/"
eng_vocab_file = data_dir+"google-10000-english/google-10000-english.txt"
bilingual_lexicon = os.environ['bilingual_lexicon']
eng_vocab_set0 = set()
with open(eng_vocab_file) as ff:
    for l in ff:
        eng_vocab_set0.add(l.strip())

### wiki stuff
data_non_domain_dir = data_dir+"1-billion-word-language-modeling-benchmark/scripts/training-monolingual.tokenized/" # for ngram lm
train_non_domain_all = data_non_domain_dir+"news.all.en" # for ngram lm
glove_dir = data_dir+"glove/" # for pagerank
index_dir = data_dir+"wiki/" # for pmi
wiki_dump_dir = data_dir+"wiki_dump/wikitext-103/" # for dclm (https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/)
wiki_dump_train = wiki_dump_dir+"wiki.train.tokens" # for dclm
wiki_dump_dev = wiki_dump_dir+"wiki.valid.tokens" # for dclm
wiki_dump_test = wiki_dump_dir+"wiki.test.tokens" # for dclm

### stopwords and punctuations
nltk_data_dir = data_dir+"nltk_data/"
import nltk
nltk.data.path.append(nltk_data_dir)
from nltk.corpus import stopwords
function_words = set(stopwords.words('english'))
import string
punctuations = set(string.punctuation)
printables = set(string.printable)
digits = set(string.digits)

### pmi numbers 
import os
pmi_dir = data_dir+"pmi_dir/"
if not os.path.exists(pmi_dir):
    os.makedirs(pmi_dir)
context_words_record_file = pmi_dir+"context_words_record"
pmi_mat_dir = pmi_dir+"pmi_mat_dir/"
if not os.path.exists(pmi_mat_dir):
    os.makedirs(pmi_mat_dir)
pmi_mat_capacity = 100

### external software stuff
moses_dir = src_dir+"mosesdecoder/" # for bound
fast_align = moses_dir+"fast_align/build/fast_align" # for bound
multi_bleu = moses_dir+"scripts/generic/multi-bleu.perl" # for general evaluation
sent_bleu = moses_dir+"bin/sentence-bleu" # for bound
meteor_bin = moses_dir+"meteor-1.5/meteor-1.5.jar" # for bound
kenlm_dir = src_dir+"kenlm/build/bin/" # for ngram lm
#lm_builder = kenlm_dir+"lmplz" # for ngram lm
#query_perplexity = kenlm_dir+"query" # for ngram lm
#build_binary = kenlm_dir+"build_binary" # for ngram lm
lm_builder = moses_dir+"bin/lmplz"
query_perplexity = moses_dir+"bin/query"
build_binary = moses_dir+"bin/build_binary"
palmetto_jar = src_dir+"Palmetto/palmetto/target/palmetto-0.1.0-jar-with-dependencies.jar" # for pmi
commons_lang_jar = src_dir+"commons-lang3-3.5/commons-lang3-3.5.jar" # used to be for pmi
hppc_jar = src_dir+"hppc/hppc/target/hppc-0.8.0-SNAPSHOT.jar" # for pmi
hypergraph_dec = src_dir+"lazy/bin/decode" # for ngram lm
wcluster = src_dir+"brown-cluster/wcluster" # for neural lm
boost_dir = src_dir+"boost_1_61_0/lib" # for dclm
dynet_dir = src_dir+"dynet/build/dynet" # for dclm
dynet_python_dir = src_dir+"dynet/build/python" # for dclm
mvn_dir = src_dir+"maven/apache-maven-3.5.0/bin" # for pmi
